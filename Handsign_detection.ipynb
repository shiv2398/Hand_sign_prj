{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f836fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 19:05:44.415186: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-12 19:05:44.415221: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.optimizers import RMSprop ,SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51dbf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/home/sahitya/Desktop/Projects/Hand_sign_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc23447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models', 'data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b436e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir='/home/sahitya/Desktop/Projects/Hand_sign_project/data/Train'\n",
    "validation_data_dir='/home/sahitya/Desktop/Projects/Hand_sign_project/data/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d6e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=24\n",
    "img_rows,img_cols=48,48\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5955b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27455 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "color_mode='grayscale',\n",
    "target_size=(img_rows,img_cols),\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4163def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7172 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen=ImageDataGenerator(rescale=1./255)\n",
    "validation_generator=validation_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "color_mode='grayscale',\n",
    "target_size=(img_rows,img_cols),\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1065d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.optimizers import RMSprop ,SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a80a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 17:28:20.065846: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-12 17:28:20.065917: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-12 17:28:20.065958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sahitya-Inspiron-15-3567): /proc/driver/nvidia/version does not exist\n",
      "2022-09-12 17:28:20.066356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 48, 48, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                147520    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                1560      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 24)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,331,192\n",
      "Trainable params: 1,328,056\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e62969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fba0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'K', 10: 'L', 11: 'M', 12: 'N', 13: 'O', 14: 'P', 15: 'Q', 16: 'R', 17: 'S', 18: 'T', 19: 'U', 20: 'V', 21: 'W', 22: 'X', 23: 'Y'}\n"
     ]
    }
   ],
   "source": [
    "class_labels=validation_generator.class_indices\n",
    "class_labels={v:k for k,v in class_labels.items()}\n",
    "classes=list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5813dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahitya/.local/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/tmp/ipykernel_42115/3659097843.py:30: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1715/1715 [==============================] - ETA: 0s - loss: 1.1382 - accuracy: 0.6310\n",
      "Epoch 00001: val_loss improved from inf to 0.21004, saving model to ./saved_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 17:36:58.741641: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/assets\n",
      "1715/1715 [==============================] - 489s 284ms/step - loss: 1.1382 - accuracy: 0.6310 - val_loss: 0.2100 - val_accuracy: 0.9275 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "1715/1715 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9027\n",
      "Epoch 00002: val_loss did not improve from 0.21004\n",
      "1715/1715 [==============================] - 478s 279ms/step - loss: 0.3064 - accuracy: 0.9027 - val_loss: 0.4078 - val_accuracy: 0.9284 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "1715/1715 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9464\n",
      "Epoch 00003: val_loss did not improve from 0.21004\n",
      "1715/1715 [==============================] - 478s 279ms/step - loss: 0.1732 - accuracy: 0.9464 - val_loss: 0.8151 - val_accuracy: 0.8124 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "1715/1715 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9535\n",
      "Epoch 00004: val_loss improved from 0.21004 to 0.10256, saving model to ./saved_models\n",
      "INFO:tensorflow:Assets written to: ./saved_models/assets\n",
      "1715/1715 [==============================] - 568s 331ms/step - loss: 0.1573 - accuracy: 0.9535 - val_loss: 0.1026 - val_accuracy: 0.9655 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "1715/1715 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9626\n",
      "Epoch 00005: val_loss improved from 0.10256 to 0.04913, saving model to ./saved_models\n",
      "INFO:tensorflow:Assets written to: ./saved_models/assets\n",
      "1715/1715 [==============================] - 709s 414ms/step - loss: 0.1202 - accuracy: 0.9626 - val_loss: 0.0491 - val_accuracy: 0.9876 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train=True\n",
    "if train:\n",
    "\n",
    "    checkpoint=ModelCheckpoint('./saved_models',\n",
    "                                  monitor='val_loss',\n",
    "                                  mode='min',\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "    earlystop=EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True)\n",
    "    reduce_lr=ReduceLROnPlateau(montor='val_loss',\n",
    "                                   factor=0.2,\n",
    "                                   patience=3,\n",
    "                                   verbose=1,\n",
    "                                   min_delata=0.001)\n",
    "    call_backs=[earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(lr=0.01),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    nb_train_samples=27455\n",
    "    nb_validation_samples=7172\n",
    "    epochs=5\n",
    "    history=model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples//batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=call_backs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b378de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27455 images belonging to 24 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46449/3239218894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_validation_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report ,confusion_matrix\n",
    "import numpy as np\n",
    "bn_train_samples=28273\n",
    "nb_validation_samples=3534\n",
    "validation_generator=validation_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "color_mode='grayscale',\n",
    "target_size=(img_rows,img_cols),\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "shuffle=True)\n",
    "class_labels=validation_generator.class_indices\n",
    "class_labels={v:k for k,v in class_labels.items()}\n",
    "classes=list(class_labels.values())\n",
    "\n",
    "y_pred=model.predict(validation_generator,nb_validation_samples//batch_size)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes,y_pred))\n",
    "print('Confusion Report')\n",
    "target_names=list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes,y_pred,target_names=target_names))\n",
    "cnf_matrix=confusion_matrix(validation_generator.classes,y_pred)\n",
    "plt.imshow(cnf_matrix,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks=np.arange(len(classes))\n",
    "plt.xticks(tick_marks,classes,rotation=90)\n",
    "plt.yticks(tick_marks,classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77842ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c732f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 19:05:57.022130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-12 19:05:57.022167: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-12 19:05:57.022200: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sahitya-Inspiron-15-3567): /proc/driver/nvidia/version does not exist\n",
      "2022-09-12 19:05:57.022504: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "classifier= keras.models.load_model('/home/sahitya/Desktop/Projects/Hand_sign_project/saved_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f53c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27455 images belonging to 24 classes.\n",
      "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'K', 10: 'L', 11: 'M', 12: 'N', 13: 'O', 14: 'P', 15: 'Q', 16: 'R', 17: 'S', 18: 'T', 19: 'U', 20: 'V', 21: 'W', 22: 'X', 23: 'Y'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator=validation_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "color_mode='grayscale',\n",
    "target_size=(img_rows,img_cols),\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "shuffle=True)\n",
    "class_labels=validation_generator.class_indices\n",
    "class_labels={v:k for k,v in class_labels.items()}\n",
    "classes=list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6272c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "import re\n",
    "\n",
    "def draw_test(name,pred,im,true_label):\n",
    "    BLACK=[0,0,0]\n",
    "    expanded_image=cv2.copyMakeBorder(im,160,0,0,300,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image,'Predicted :- '+pred,(120,60),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    cv2.putText(expanded_image,'True :- '+true_label,(60,120),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "    cv2.imshow(name,expanded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c27a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomImage(path,img_width,img_height):\n",
    "    folders=list(filter(lambda x:os.path.isdir(os.path.join(path,x)),os.listdir(path)))\n",
    "    random_directory=np.random.randint(0,len(folders))\n",
    "    path_class=folders[random_directory]\n",
    "    file_path=path+path_class\n",
    "    file_names=[f for f in listdir(file_path) if isfile(join(file_path,f))]\n",
    "    random_file_index=np.random.randint(0,len(file_names))\n",
    "    image_name=file_names[random_file_index]\n",
    "    final_path=file_path+'/'+image_name\n",
    "    return image.load_img(final_path,target_size=(img_width,img_height),grayscale=True),final_path,path_class\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "981adef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[4]\n",
      "[15]\n",
      "[1]\n",
      "[8]\n",
      "[19]\n",
      "[8]\n",
      "[17]\n",
      "[1]\n",
      "[19]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "img_width,img_height=48,48\n",
    "classifier.compile(loss='categorical_crossentropy',\n",
    "             optimizer=RMSprop(lr=0.001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "files=[]\n",
    "prediction=[]\n",
    "true_labels=[]\n",
    "\n",
    "for i in range(0,10):\n",
    "    path='/home/sahitya/Desktop/Projects/Hand_sign_project/data/Test/'\n",
    "    img,final_path,true_label=getRandomImage(path,img_width,img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x=image.img_to_array(img)\n",
    "    x=x*1.0/255\n",
    "    x=np.expand_dims(x,axis=0)\n",
    "    images=np.vstack([x])\n",
    "    pred=classifier.predict(images,batch_size=10)\n",
    "    classes=np.argmax(pred,axis=1)\n",
    "    print(classes)\n",
    "    prediction.append(classes)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab55f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(files)):\n",
    "    image=cv2.imread((files[i]))\n",
    "    image=cv2.resize(image,None,fx=3,fy=3,interpolation=cv2.INTER_CUBIC)\n",
    "    draw_test('Prediction-->:',class_labels[prediction[i][0]],image,true_labels[i])\n",
    "    cv2.waitKey()  \n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f059ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64181e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
